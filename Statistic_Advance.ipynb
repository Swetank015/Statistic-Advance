{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Q1. Explain the properties of the F-distribution.**"
      ],
      "metadata": {
        "id": "ZySgSyzdGLTo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The F-distribution, also known as the Fisher-Snedecor distribution, is a continuous probability distribution that arises frequently in statistical analysis. It is characterized by the following properties:\n",
        "\n",
        "1. **Shape:**\n",
        "\n",
        "* Right-skewed: The F-distribution is always skewed to the right, meaning it has a long tail on the right side.\n",
        "\n",
        "* Varying shape: The exact shape of the distribution depends on the degrees of freedom associated with the numerator and denominator of the F-statistic.\n",
        "\n",
        "2. **Range:**\n",
        "\n",
        "* Non-negative: The F-statistic can only take on non-negative values (greater than or equal to zero).\n",
        "\n",
        "3. **Degrees of Freedom:**\n",
        "\n",
        "* Two parameters: The F-distribution is defined by two parameters, which are the degrees of freedom for the numerator (df1) and the denominator (df2).\n",
        "\n",
        "* Shape influence: The values of df1 and df2 significantly impact the shape of the distribution. As both df1 and df2 increase, the F-distribution approaches a normal distribution.\n",
        "\n",
        "4. **Applications:**\n",
        "\n",
        "* Analysis of Variance (ANOVA): The F-distribution is used to compare the variances of two or more populations in ANOVA.\n",
        "\n",
        "* Regression Analysis: It is used to test the overall significance of a regression model and to compare the fit of different models.\n",
        "Hypothesis Testing: The F-test is used to test hypotheses about population variances.\n",
        "\n",
        "5. **F-Statistic:**\n",
        "\n",
        "* Ratio of variances: The F-statistic is calculated as the ratio of two independent chi-square variables, each divided by its respective degrees of freedom.\n",
        "\n",
        "* Null hypothesis: Under the null hypothesis, the F-statistic follows an F-distribution with the specified degrees of freedom.\n",
        "\n",
        "**Key Points to Remember:**\n",
        "\n",
        "* The F-distribution is a continuous probability distribution used in various statistical tests.\n",
        "\n",
        "* It is right-skewed and its shape depends on the degrees of freedom.\n",
        "\n",
        "* The F-statistic is a ratio of two variances and is used in hypothesis testing.\n",
        "\n",
        "* As the degrees of freedom increase, the F-distribution approaches a normal distribution.\n"
      ],
      "metadata": {
        "id": "ChnqNv6tGsg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "B0-I4HUkIMHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.The F-distribution is primarily used in two major statistical tests:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA):**\n",
        "\n",
        "* Purpose: ANOVA is used to compare the means of two or more groups to determine if there are significant differences between them.\n",
        "\n",
        "* Why F-distribution is appropriate: The F-statistic in ANOVA is calculated as the ratio of the variance between groups to the variance within groups. Under the null hypothesis (that all group means are equal), this ratio follows an F-distribution. A significant F-statistic indicates that the differences between group means are unlikely to be due to chance.\n",
        "\n",
        "2. Testing Equality of Variances:\n",
        "\n",
        "* Purpose: This test is used to determine if the variances of two populations are equal.\n",
        "\n",
        "* Why F-distribution is appropriate: The F-statistic in this test is calculated as the ratio of the larger sample variance to the smaller sample variance. Under the null hypothesis of equal variances, this ratio follows an F-distribution. A significant F-statistic suggests that the variances are likely different.\n",
        "\n",
        "**Key points to remember:**\n",
        "\n",
        "* The F-distribution is appropriate for these tests because it arises naturally from the ratio of variances, which is a fundamental concept in both ANOVA and variance comparison.\n",
        "\n",
        "* The F-statistic, calculated from the data, is compared to a critical value from the F-distribution to make a decision about the null hypothesis.\n",
        "\n",
        "* The degrees of freedom associated with the numerator and denominator of the F-statistic determine the specific shape of the distribution.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1LlkXv--IWZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?**"
      ],
      "metadata": {
        "id": "Qj74MZllKhSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.To conduct an F-test to compare the variances of two populations, the following key assumptions must be met:\n",
        "\n",
        "1. Independence: The two samples must be independent of each other. This means that the selection of one sample should not influence the selection of the other.\n",
        "\n",
        "2. Normality: Both populations from which the samples are drawn should be normally distributed. This assumption is crucial for the validity of the F-test, as it relies on the properties of the normal distribution.\n",
        "\n",
        "3. Equal Variances (Homoscedasticity): This assumption is a bit counterintuitive, as the F-test is specifically designed to test for equal variances. However, the F-test assumes that the null hypothesis of equal variances is true. If this assumption is violated, the results of the F-test may be unreliable.\n",
        "\n",
        "It's important to note that the F-test is sensitive to violations of these assumptions, especially the normality assumption. If the data is not normally distributed, alternative tests like Levene's test or Bartlett's test can be used to compare variances. Additionally, if the sample sizes are large, the Central Limit Theorem can help mitigate the impact of non-normality."
      ],
      "metadata": {
        "id": "ba1mVs0oKtbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4.What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "A79-4AIGMsZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. ANOVA (Analysis of Variance) and the t-test are both statistical tests used to compare means, but they serve different purposes and are applied in different scenarios. Here's a breakdown of their purposes and how they differ:\n",
        "\n",
        "**Purpose of ANOVA:**\n",
        "\n",
        "* ANOVA is used to compare the means of three or more groups to determine if at least one of the group means is significantly different from the others. It tests the hypothesis that all groups have the same population mean, against the alternative that at least one group mean differs.\n",
        "\n",
        "* ANOVA helps you understand whether there is a significant difference between groups in a situation where you have more than two categories or treatments to compare (e.g., comparing test scores between three different teaching methods).\n",
        "\n",
        "* The key output of ANOVA is an F-statistic, which tests for variance between group means relative to the variance within the groups. If the F-statistic is large and the p-value is small, you can reject the null hypothesis and conclude that at least one group is different.\n",
        "\n",
        "Purpose of the t-test:\n",
        "\n",
        "* A t-test is typically used to compare the means of two groups to see if they are significantly different from each other. It can be used for independent samples (e.g., comparing two different treatment groups) or paired samples (e.g., comparing before and after measurements within the same group).\n",
        "\n",
        "* The key output of a t-test is a t-statistic, which is based on the difference between the group means relative to the variability in the data.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "1. Number of Groups:\n",
        "\n",
        "* ANOVA: Used for comparing means across three or more groups.\n",
        "\n",
        "* t-test: Used for comparing the means of two groups.\n",
        "\n",
        "2. Null Hypothesis:\n",
        "\n",
        "* ANOVA: Tests if all group means are equal. The null hypothesis states that all group means are the same.\n",
        "\n",
        "* t-test: Tests if two group means are equal. The null hypothesis states that the two means are the same.\n",
        "\n",
        "3. Statistical Output:\n",
        "\n",
        "* ANOVA: Provides an F-statistic, which compares the variance between group means to the variance within groups.\n",
        "\n",
        "* t-test: Provides a t-statistic, which measures the difference between two group means relative to the standard error of the difference.\n",
        "\n",
        "4. Post-hoc Tests:\n",
        "\n",
        "* ANOVA: If ANOVA finds a significant difference, post-hoc tests (e.g., Tukey's HSD) are often performed to identify which specific groups are different.\n",
        "\n",
        "* t-test: Since a t-test compares only two groups, there is no need for post-hoc tests unless multiple pairwise comparisons are made.\n",
        "\n",
        "5. Assumptions:\n",
        "\n",
        "* Both tests assume that data is approximately normally distributed, that variances are homogeneous (equal across groups), and that observations are independent. However, ANOVA can handle more complex situations with multiple groups, while the t-test is simpler and more direct for two-group comparisons."
      ],
      "metadata": {
        "id": "X7Xfz_HrMyTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.**"
      ],
      "metadata": {
        "id": "Y0MbsegHP8vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans.When comparing the means of more than two groups, the choice between a one-way ANOVA and multiple t-tests is crucial. Let's break down the scenarios where each is appropriate:\n",
        "\n",
        "One-way ANOVA\n",
        "\n",
        "* Scenario: You want to compare the means of three or more independent groups on a single dependent variable.\n",
        "\n",
        "**Why use it:**\n",
        "\n",
        "* Controls Type I Error Rate: By performing a single overall test, ANOVA maintains a consistent Type I error rate (the probability of incorrectly rejecting a true null hypothesis).\n",
        "\n",
        "* More Powerful: ANOVA is generally more powerful than multiple t-tests, especially when sample sizes are unequal or group variances differ.\n",
        "\n",
        "* Efficient: It provides a single p-value to assess the overall significance of group differences.\n",
        "\n",
        "Multiple t-tests\n",
        "\n",
        "* Scenario: You want to compare the means of specific pairs of groups.\n",
        "\n",
        "* Why use it:\n",
        "\n",
        "* Targeted Comparisons: If you have a specific hypothesis about which pairs of groups differ, multiple t-tests can directly address those comparisons.\n",
        "\n",
        "* Flexibility: You can tailor your analysis to the specific questions of interest.\n",
        "\n",
        "**Key Considerations:**\n",
        "\n",
        "1. Type I Error Rate:\n",
        "\n",
        "* Multiple t-tests: Conducting multiple t-tests increases the overall Type I error rate, as each test has a chance of incorrectly rejecting a null hypothesis.\n",
        "\n",
        "* One-way ANOVA: By controlling the family-wise error rate, ANOVA mitigates this issue.\n",
        "\n",
        "2. Power:\n",
        "\n",
        "* One-way ANOVA: Generally more powerful, especially when sample sizes are unequal or variances differ.\n",
        "\n",
        "3. Post-hoc Tests:\n",
        "\n",
        "* If the one-way ANOVA is significant, post-hoc tests (like Tukey's HSD or Bonferroni) can be used to identify which specific pairs of groups differ significantly."
      ],
      "metadata": {
        "id": "clo8qnJVQFlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q6.Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "PV5WYGc-EcpZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. **Partitioning Variance in ANOVA**\n",
        "\n",
        "**In ANOVA, the total variance in a dataset is partitioned into two components:**\n",
        "\n",
        "1. Between-Group Variance: This measures the variability between the means of different groups. It represents the differences among the group means.\n",
        "\n",
        "2. Within-Group Variance: This measures the variability within each group. It represents the random variation within each group.\n",
        "\n",
        "**Calculating the F-Statistic**\n",
        "\n",
        "**The F-statistic is a ratio of these two variances:**\n",
        "\n",
        "F = (Between-Group Variance) / (Within-Group Variance)\n",
        "\n",
        "\n",
        "* Numerator (Between-Group Variance): If the group means are significantly different, the between-group variance will be large.\n",
        "\n",
        "* Denominator (Within-Group Variance): This represents the inherent variability within each group, regardless of the group membership.\n",
        "\n",
        "**Interpreting the F-Statistic**\n",
        "\n",
        "\n",
        "* Large F-statistic: Indicates that the between-group variance is significantly larger than the within-group variance, suggesting that the group means are likely different.\n",
        "\n",
        "* Small F-statistic: Suggests that the differences between group means are not significant, and the observed differences could be due to random chance.\n",
        "\n",
        "Why is this Partitioning Important?\n",
        "\n",
        "By partitioning the variance, ANOVA allows us to:\n",
        "\n",
        "* Identify significant differences: Determine whether the differences between group means are statistically significant.\n",
        "\n",
        "* Understand the sources of variation: Identify the factors contributing to the overall variability in the data.\n",
        "\n",
        "* Make informed decisions: Use the results to draw conclusions about the population(s) from which the sample was drawn."
      ],
      "metadata": {
        "id": "XW4trZvfEzQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "WFo9zjJbP81H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The classical (frequentist) and Bayesian approaches to analysis of variance (ANOVA) differ significantly in how they handle uncertainty, parameter estimation, and hypothesis testing. Below is a comparison of these two frameworks:\n",
        "\n",
        "### 1. **Handling Uncertainty**\n",
        "   - **Frequentist Approach:**\n",
        "     - In the frequentist framework, uncertainty is modeled based on the idea of repeated sampling. The parameters are considered fixed but unknown, and the data are viewed as random.\n",
        "     - Uncertainty is expressed through **confidence intervals** and **p-values**, which are based on the sampling distribution of the estimator.\n",
        "     - A frequentist does not assign probability to the parameters themselves. The probability in frequentist statistics pertains to the data given the parameters (i.e., the likelihood).\n",
        "\n",
        "   - **Bayesian Approach:**\n",
        "     - In the Bayesian framework, uncertainty is handled by treating the parameters as random variables with prior distributions. The prior captures what is known about the parameters before observing the data, and the likelihood represents how the data are generated given the parameters.\n",
        "     - Uncertainty is expressed through **posterior distributions**. After observing data, the posterior distribution updates the prior belief based on the evidence in the data.\n",
        "     - The Bayesian approach allows direct probabilistic statements about the parameters, such as \"there is a 95% probability that the true mean lies within this range.\"\n",
        "\n",
        "### 2. **Parameter Estimation**\n",
        "   - **Frequentist Approach:**\n",
        "     - In frequentist ANOVA, parameter estimates (such as means and variances) are obtained through **maximum likelihood estimation (MLE)** or method of moments.\n",
        "     - The focus is on finding point estimates (e.g., sample means) that are most likely given the data, and these estimates are considered fixed values that are subject to sampling variability.\n",
        "     - Confidence intervals are typically used to express the precision of these estimates.\n",
        "\n",
        "   - **Bayesian Approach:**\n",
        "     - In Bayesian ANOVA, parameter estimation is done by computing the **posterior distribution** of the parameters. The posterior combines prior knowledge and the likelihood of the data.\n",
        "     - Bayesian inference does not just provide point estimates (such as a sample mean) but also a full distribution over the possible values of the parameters, from which you can derive summaries like the mean, median, credible intervals, etc.\n",
        "     - A common measure of central tendency in the Bayesian framework is the **posterior mean** (or sometimes the **posterior median**), and uncertainty is captured by the spread of the posterior distribution.\n",
        "\n",
        "### 3. **Hypothesis Testing**\n",
        "   - **Frequentist Approach:**\n",
        "     - In frequentist ANOVA, hypothesis testing involves comparing the **null hypothesis** (typically that there are no differences between group means) to the alternative hypothesis (that there are differences).\n",
        "     - The test statistic (e.g., F-statistic) is computed, and the p-value is derived to assess the strength of evidence against the null hypothesis.\n",
        "     - If the p-value is less than a significance threshold (e.g., 0.05), the null hypothesis is rejected. The frequentist approach focuses on controlling **Type I** and **Type II errors**.\n",
        "\n",
        "   - **Bayesian Approach:**\n",
        "     - In Bayesian ANOVA, hypothesis testing is typically framed as evaluating the **posterior probability** of different hypotheses or models. Instead of a single p-value, Bayesian hypothesis testing often involves comparing models or hypotheses using **Bayes factors**, which quantify the relative evidence for one hypothesis over another.\n",
        "     - The Bayes factor compares the likelihood of the data under two competing hypotheses, where values greater than 1 suggest evidence in favor of the alternative hypothesis, and values less than 1 suggest evidence in favor of the null hypothesis.\n",
        "     - Bayesian tests allow for a more flexible and probabilistic interpretation of the hypotheses (e.g., \"There is a 95% probability that the true difference between groups is greater than 0\").\n",
        "\n",
        "### 4. **Interpretation of Results**\n",
        "   - **Frequentist Approach:**\n",
        "     - In frequentist ANOVA, the interpretation revolves around long-run properties of estimators and test statistics. Results are framed in terms of confidence intervals and p-values that reflect the probability of observing the data, assuming the null hypothesis is true.\n",
        "     - The conclusion from a frequentist test is often binary: reject or fail to reject the null hypothesis based on the p-value.\n",
        "   \n",
        "   - **Bayesian Approach:**\n",
        "     - In Bayesian ANOVA, results are interpreted in terms of probability distributions. The posterior distribution provides a probabilistic estimate of the parameters, and hypothesis testing focuses on the probability of the null hypothesis or the parameters of interest, given the observed data.\n",
        "     - Bayesians can make direct statements about parameter values, such as \"There is a 95% probability that the true mean difference between groups is between -2 and 3.\"\n",
        "\n",
        "### 5. **Model Comparison and Complexity**\n",
        "   - **Frequentist Approach:**\n",
        "     - Frequentist methods for model comparison often rely on **likelihood ratios**, AIC (Akaike Information Criterion), or BIC (Bayesian Information Criterion) to compare models.\n",
        "     - Model selection typically focuses on minimizing residuals or maximizing the fit to the data.\n",
        "   \n",
        "   - **Bayesian Approach:**\n",
        "     - In Bayesian statistics, model comparison is done using the **Bayes factor**, or the **posterior predictive check** to evaluate how well different models predict the observed data.\n",
        "     - A Bayesian approach can naturally handle **model uncertainty**, as the posterior distribution can incorporate various models or parameters.\n",
        "\n",
        "### 6. **Assumptions and Flexibility**\n",
        "   - **Frequentist Approach:**\n",
        "     - Frequentist ANOVA assumes that data are drawn from a certain distribution (typically normal), and it requires the assumptions of homogeneity of variances and independence of observations.\n",
        "     - These assumptions are critical for the validity of results. If the assumptions are violated, the conclusions from a frequentist ANOVA may be invalid.\n",
        "   \n",
        "   - **Bayesian Approach:**\n",
        "     - Bayesian methods are more flexible with respect to modeling assumptions. Priors can be chosen to reflect any reasonable belief or assumption about the data, and the model can incorporate more complex structures (e.g., hierarchical models, non-normal distributions).\n",
        "     - Bayesian methods are less sensitive to small violations of assumptions, especially when the model is correctly specified.\n",
        "\n",
        "### Summary of Key Differences:\n",
        "\n",
        "| Aspect                       | Frequentist ANOVA                          | Bayesian ANOVA                        |\n",
        "|------------------------------|--------------------------------------------|---------------------------------------|\n",
        "| **Uncertainty**               | Uncertainty is reflected in confidence intervals and p-values, based on the sampling distribution. | Uncertainty is modeled through the posterior distribution of parameters. |\n",
        "| **Parameter Estimation**      | Point estimates (e.g., sample mean), and confidence intervals. | Posterior distribution, providing a full range of possible parameter values. |\n",
        "| **Hypothesis Testing**        | Null hypothesis significance testing (p-values) to accept/reject hypotheses. | Bayesian hypothesis testing, Bayes factors, posterior probabilities. |\n",
        "| **Interpretation**            | Long-run properties, such as Type I and II errors. | Probabilistic interpretation of parameters and models. |\n",
        "| **Flexibility**               | Limited to classical assumptions (e.g., normality, homogeneity of variances). | More flexible, allows incorporation of complex models and priors. |\n",
        "\n",
        "In summary, while the frequentist approach focuses on using data to make decisions about the parameters based on sampling distributions and long-run behavior, the Bayesian approach emphasizes updating beliefs about parameters in light of the data, providing a more flexible, probabilistic framework for analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "S_uatngQQKVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q8. Question: You have two sets of data representing the incomes of two different professions:**\n",
        "\n",
        "* Profession A: [48, 52, 55, 60, 62]\n",
        "* Profession B: [45, 50, 55, 52, 47] **Perform an F-test to determine if the variances of the two professions incomes are equal. What are your conclusions based on the F-test?**\n",
        "\n",
        "**Task:** Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "**Objective:** Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "vYrNlFJvUFGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Steps for performing an F-test:\n",
        "\n",
        "1. State the null hypothesis (Hâ‚€):\n",
        "\n",
        "* Hâ‚€: The variances of the two professions' incomes are equal (Ïƒâ‚Â² = Ïƒâ‚‚Â²).\n",
        "\n",
        "2. State the alternative hypothesis (Hâ‚):\n",
        "\n",
        "* Hâ‚: The variances of the two professions' incomes are not equal (Ïƒâ‚Â² â‰  Ïƒâ‚‚Â²).\n",
        "\n",
        "\n",
        "3. F-statistic formula: The F-statistic is the ratio of the variances of the two samples:\n",
        "\n",
        "ð¹\n",
        "=\n",
        "ð‘ \n",
        "1\n",
        "2\n",
        "ð‘ \n",
        "2\n",
        "2\n",
        "F=\n",
        "s\n",
        "2\n",
        "2\n",
        "â€‹\n",
        "\n",
        "s\n",
        "1\n",
        "2\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "where\n",
        "ð‘ \n",
        "1\n",
        "2\n",
        "s\n",
        "1\n",
        "2\n",
        "â€‹\n",
        "  is the variance of Profession A, and\n",
        "ð‘ \n",
        "2\n",
        "2\n",
        "s\n",
        "2\n",
        "2\n",
        "â€‹\n",
        "  is the variance of Profession B.\n",
        "\n",
        "* Calculate the p-value using the F-distribution:\n",
        "\n",
        "* The p-value tells you the probability of observing the F-statistic (or one more extreme) under the null hypothesis.\n",
        "\n",
        "Python Code to Perform the F-test:\n",
        "\n",
        "We can use the scipy.stats library to perform the F-test in Python."
      ],
      "metadata": {
        "id": "oI6rmZxVU4tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "\n",
        "F_stat = var_A / var_B if var_A > var_B else var_B / var_A\n",
        "\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "p_value = 1 - stats.f.cdf(F_stat, df_A, df_B)\n",
        "\n",
        "print(f\"F-statistic: {F_stat:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are not significantly different.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8fsHippYYDl",
        "outputId": "ca56850b-a529-451a-f5c3-4845f88f8c19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089\n",
            "P-value: 0.247\n",
            "Fail to reject the null hypothesis: The variances are not significantly different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:**\n",
        "\n",
        "* **Region A: [160, 162, 165, 158, 164]**\n",
        "* **Region B: [172, 175, 170, 168, 174]**\n",
        "* **Region C: [180, 182, 179, 185, 183]**\n",
        "\n",
        "* **Task: Write Python code to perform the one-way ANOVA and interpret the results**\n",
        "* **Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.**"
      ],
      "metadata": {
        "id": "He6pgegOZdvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "region_A = np.array([160, 162, 165, 158, 164])\n",
        "region_B = np.array([172, 175, 170, 168, 174])\n",
        "region_C = np.array([180, 182, 179, 185, 183])\n",
        "\n",
        "F_stat, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "print(f\"F-statistic: {F_stat:.3f}\")\n",
        "print(f\"P-value: {p_value:.3f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a statistically significant difference in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no statistically significant difference in average heights between the regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EDVqJyCbcYV",
        "outputId": "e9ceb54a-4e0a-44ca-906a-ed058bd6ec23"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.873\n",
            "P-value: 0.000\n",
            "Reject the null hypothesis: There is a statistically significant difference in average heights between the regions.\n"
          ]
        }
      ]
    }
  ]
}